{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze DTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "# -----------------------------------------------------------------------------\n",
    "# import pip\n",
    "# pip.main(['install', 'seaborn']) \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "running_on = 'server' if 'Linux' in platform.system() else 'my_mac'\n",
    "\n",
    "if running_on == 'my_mac':\n",
    "    data_path = '/Users/ranigera/Dropbox/DTI_tests'\n",
    "    preproc_path = '/Users/ranigera/Dropbox/DTI_tests/preproc'\n",
    "    launch_files_path = '/Users/ranigera/Dropbox/DTI_analysis/launch_files'\n",
    "else:\n",
    "    data_path = '/export2/DATA/HIS/HIS_server/BIDS'\n",
    "    preproc_path = '/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc'\n",
    "    launch_files_path = '/export2/DATA/HIS/HIS_server/codes_dwi/launch_files'\n",
    "\n",
    "expectedVolums = {\n",
    "    'AP': 69,\n",
    "    'PA' : 7,\n",
    "    }\n",
    "expectedB0s_indxs = {\n",
    "    'AP_before': [0, 1, 18, 35, 52],\n",
    "    'PA_before': [0, 2, 3, 4, 5, 6],\n",
    "    'AP_after': [0, 1, 18, 35, 52],\n",
    "    'PA_after': [0, 2, 3, 4, 5, 6]\n",
    "    }\n",
    "\n",
    "n_cores_TOPUP = 4\n",
    "\n",
    "EDDY_command = 'eddy_openmp' if running_on == 'server' else 'eddy'\n",
    "n_expected_EDDY_output_files = 13\n",
    "n_cores_EDDY = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Get sub folders\n",
      ">> Get exclusion list\n",
      ">> Remove sub folders of excluded participants in case they are there\n"
     ]
    }
   ],
   "source": [
    "# Get folders and remove excluded subjects\n",
    "# -----------------------------------------------------------------------------\n",
    "print('>> Get sub folders')\n",
    "subjFolders = [el for el in os.listdir(data_path) if 'sub' in el]\n",
    "\n",
    "if running_on == 'my_mac':\n",
    "    print('>> Get exclusion list')\n",
    "    with open('/Users/ranigera/Google_Drive_TAU/Experiments/HIS_STUDY/Analysis/codes/paths_and_vars.py') as txtFile:\n",
    "        txt = txtFile.read()\n",
    "    participantsToExclude = [int(el) for el in txt.split('participantsToExclude = [')[1].split(']')[0].replace('\\n','').replace('\\n','').replace(\"'\",\"\").split(',')]\n",
    "\n",
    "    print('>> Remove sub folders of excluded participants in case they are there')\n",
    "    subjFolders = [el for el in subjFolders if int(el.split('-')[1]) not in participantsToExclude]\n",
    "\n",
    "subjFolders.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing scans or wrong phase encoding directions for ALL SUBJECTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Verify that all the scans exist and that the phase encoding directions are as they should.\n"
     ]
    }
   ],
   "source": [
    "print ('>> Verify that all the scans exist and that the phase encoding directions are as they should.')\n",
    "subjectsWithAProblem = []\n",
    "for subjFolder in subjFolders:\n",
    "    sub = int(subjFolder.split(\"-\",1)[1])\n",
    "    group = '1day' if sub < 200 else '3day'\n",
    "    last_sess = group[0]\n",
    "    DWI_path_before = os.path.join(data_path, subjFolder, 'ses-1/dwi/')\n",
    "    DWI_path_after = os.path.join(data_path, subjFolder, f'ses-{last_sess}/dwi/')\n",
    "    scansBaseNames = {\n",
    "        'AP_before': f'{os.path.join(DWI_path_before, \"sub-\" + str(sub) + \"_ses-1_acq-ap_run-01_dwi\")}',\n",
    "        'PA_before' : f'{os.path.join(DWI_path_before, \"sub-\" + str(sub) + \"_ses-1_acq-pa_run-01_dwi\")}',\n",
    "        'AP_after' : f'{os.path.join(DWI_path_after, \"sub-\" + str(sub) + \"_ses-\" + last_sess + \"_acq-ap_run-02_dwi\")}',\n",
    "        'PA_after' : f'{os.path.join(DWI_path_after, \"sub-\" + str(sub) + \"_ses-\" + last_sess + \"_acq-pa_run-02_dwi\")}'\n",
    "        }\n",
    "\n",
    "    for scan in scansBaseNames.keys():\n",
    "        # print(scansBaseNames[scan] + '.json')\n",
    "        # print(scanData['PhaseEncodingDirection'])\n",
    "        if not os.path.exists(scansBaseNames[scan] + '.json'):\n",
    "            subjectsWithAProblem.append(sub)\n",
    "            print(' *** Scan not found: ' + scansBaseNames[scan] + '.json')\n",
    "            continue\n",
    "        with open(scansBaseNames[scan] + '.json') as json_file:        \n",
    "            scanData = json.load(json_file)\n",
    "            if ('acq-ap_' in scansBaseNames[scan] and scanData['PhaseEncodingDirection'] != 'j-') or \\\n",
    "                ('acq-pa_' in scansBaseNames[scan] and scanData['PhaseEncodingDirection'] != 'j'):\n",
    "                subjectsWithAProblem.append(sub)\n",
    "                print(' *** There is a problem with the scanning directions: ' + scansBaseNames[scan] + '.json is defined as ' + scanData['PhaseEncodingDirection'] + '.')\n",
    "                continue\n",
    "\n",
    "subjectsWithAProblem = list(set(subjectsWithAProblem))\n",
    "subjectsWithAProblem.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TEMP] Working on subject (for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get b0 volume indices and perform bval & bvec QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('> Verify that data points in the bval files is as expected.')\n",
    "if pd.read_csv(scansBaseNames['AP_before'] + '.bval', header=None, sep=' ').T.shape[0] != expectedVolums['AP'] or \\\n",
    "    pd.read_csv(scansBaseNames['PA_before'] + '.bval', header=None, sep=' ').T.shape[0] != expectedVolums['PA'] or \\\n",
    "    pd.read_csv(scansBaseNames['AP_after'] + '.bval', header=None, sep=' ').T.shape[0] != expectedVolums['AP'] or \\\n",
    "    pd.read_csv(scansBaseNames['PA_after'] + '.bval', header=None, sep=' ').T.shape[0] != expectedVolums['PA']:\n",
    "        print(f' *** The number of data points in the bval for one of the scans for subjetc {sub} is not as expected.')\n",
    "        raise Exception(f'The number of data points in the bval for one of the scans for subjetc {sub} is not as expected.')\n",
    "\n",
    "print('> Verify that data points in the bvec files is as expected.')\n",
    "if pd.read_csv(scansBaseNames['AP_before'] + '.bvec', header=None, sep=' ').T.shape[0] != expectedVolums['AP'] or \\\n",
    "    pd.read_csv(scansBaseNames['PA_before'] + '.bvec', header=None, sep=' ').T.shape[0] != expectedVolums['PA'] or \\\n",
    "    pd.read_csv(scansBaseNames['AP_after'] + '.bvec', header=None, sep=' ').T.shape[0] != expectedVolums['AP'] or \\\n",
    "    pd.read_csv(scansBaseNames['PA_after'] + '.bvec', header=None, sep=' ').T.shape[0] != expectedVolums['PA']:\n",
    "        print(f' *** The number of data points in the bvec for one of the scans for subjetc {sub} is not as expected.')\n",
    "        raise ValueError(f'The number of data points in the bvec for one of the scans for subjetc ' + str(sub) + ' is not as expected.')\n",
    "\n",
    "\n",
    "print('> Extract B0s:')\n",
    "B0s_indxs = {}\n",
    "for scan in scansBaseNames.keys():\n",
    "    B0s=pd.read_csv(scansBaseNames[scan] + '.bval', header=None, sep=' ').T\n",
    "    B0s.columns = ['bval']\n",
    "    B0s_indxs[scan] = list(B0s[B0s.bval < 20].index)\n",
    "\n",
    "print('> Verify that b0 quantity and indices are as expected.')\n",
    "if B0s_indxs != expectedB0s_indxs:\n",
    "    print(f' *** The indices of the b0s for one of the scans for subjetc {sub} are not as expected.')\n",
    "    raise ValueError(f'The indices of the b0s for one of the scans for subjetc ' + str(sub) + ' are not as expected.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create pre-processing folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create preprocessing folder\n"
     ]
    }
   ],
   "source": [
    "print('> Create preprocessing folder')\n",
    "try:\n",
    "    os.makedirs(os.path.join(preproc_path, subjFolder), exist_ok=False)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) TOPUP\n",
    "* A tool for estimating and correcting susceptibility induced distortions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract B0 volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Extract B0s (using the fslroi):\n",
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "print('> Extract B0s (using the fslroi):')\n",
    "sub_B0s_files = []\n",
    "for scan in scansBaseNames.keys():\n",
    "    for B0ind in B0s_indxs[scan]:\n",
    "        B0_file_name = os.path.join(preproc_path, subjFolder, subjFolder + '_' + scan + \"_b0_volInd-\" + str(B0ind) + \".nii.gz\")\n",
    "        sub_B0s_files.append(B0_file_name)\n",
    "        if not os.path.isfile(f\"{B0_file_name}\"):\n",
    "            print(f'>> runs: fslroi {scansBaseNames[scan]}.nii.gz {B0_file_name} {B0ind} 1')\n",
    "            os.system(f'fslroi {scansBaseNames[scan]}.nii.gz {B0_file_name} {B0ind} 1')\n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge B0 volumes for TOPUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Merge B0s for each session (using the fslmerge):\n",
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "print('> Merge B0s for each session (using the fslmerge):')\n",
    "#print([scan for scan in sub_B0s_files if 'AP_before' in scan])\n",
    "#print([scan for scan in sub_B0s_files if 'PA_before' in scan])\n",
    "for time in ['before', 'after']:\n",
    "    output_base_name = f'{os.path.join(preproc_path, subjFolder,subjFolder)}_AP_PA_{time}_b0s'\n",
    "    if not os.path.isfile(f\"{output_base_name}.nii.gz\"):\n",
    "        print(f'>> runs: fslmerge -t {output_base_name}' + ' ' + ' '.join([scan for scan in sub_B0s_files if f'AP_{time}' in scan]) + ' ' + ' '.join([scan for scan in sub_B0s_files if f'PA_{time}' in scan]))\n",
    "        os.system(f'fslmerge -t {output_base_name}' + ' ' + ' '.join([scan for scan in sub_B0s_files if f'AP_{time}' in scan]) + ' ' + ' '.join([scan for scan in sub_B0s_files if f'PA_{time}' in scan]))\n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the acqparams.txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create the acqparams.txt files (one per session [before and after])\n"
     ]
    }
   ],
   "source": [
    "print('> Create the acqparams.txt files (one per session [''before'' and ''after''])')\n",
    "# first get the totalReadoutTime from the json files\n",
    "total_readout_time = {}\n",
    "for scan in scansBaseNames.keys():\n",
    "    with open(scansBaseNames[scan] + '.json') as json_file:        \n",
    "        scanData = json.load(json_file)\n",
    "        total_readout_time[scan] = scanData['TotalReadoutTime']\n",
    "\n",
    "for time in ['before', 'after']:\n",
    "    acqPars=[f'0 -1 0 {total_readout_time[f\"AP_{time}\"]}' for scan in sub_B0s_files if f'AP_{time}' in scan] + [f'0 1 0 {total_readout_time[f\"PA_{time}\"]}' for scan in sub_B0s_files if f'PA_{time}' in scan]\n",
    "    # write a list of strings to a file (one string per line):\n",
    "    with open(os.path.join(preproc_path, subjFolder, subjFolder + f'_{time}_acqparams.txt'), 'w') as f:\n",
    "        for item in acqPars:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run TOPUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create TOPUP launch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create the TOPUP launch files (one per subject [before and after] together)\n",
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "print('> Create the TOPUP launch files (one per subject [''before'' and ''after''] together)')\n",
    "\n",
    "launch_file = os.path.join(launch_files_path, 'TOPUP_' + subjFolder + '_launch.txt')\n",
    "\n",
    "if not os.path.isfile(launch_file):\n",
    "    with open(launch_file, 'w') as f:\n",
    "        for time in ['before', 'after']:\n",
    "                print(f\">> This command was written to {launch_file}:\\ntopup --imain={os.path.join(preproc_path, subjFolder,subjFolder)}_AP_PA_{time}_b0s \\\\\\n\\\n",
    "                        --datain={os.path.join(preproc_path, subjFolder, subjFolder + f'_{time}_acqparams.txt')} \\\\\\n\\\n",
    "                        --config=b02b0.cnf \\\\\\n\\\n",
    "                        --out={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s \\\\\\n\\\n",
    "                        --iout={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_iout \\\\\\n\\\n",
    "                        --fout={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_fout \\n\\\n",
    "                    \")\n",
    "                f.write(f\"topup --imain={os.path.join(preproc_path, subjFolder,subjFolder)}_AP_PA_{time}_b0s --datain={os.path.join(preproc_path, subjFolder, subjFolder + f'_{time}_acqparams.txt')} --config=b02b0.cnf --out={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s --iout={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_iout --fout={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_fout\\n\")\n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run TOPUP launch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Run the TOPUP launch files (one per subject [before and after] together)\n",
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "print('> Run the TOPUP launch files (one per subject [''before'' and ''after''] together)')\n",
    "\n",
    "launch_file = os.path.join(launch_files_path, 'TOPUP_' + subjFolder + '_launch.txt')\n",
    "\n",
    "# check if all expected output files exist:\n",
    "expected_TOPUP_out_files_exist = []\n",
    "for time in ['before', 'after']:\n",
    "    expected_TOPUP_out_files_exist += [os.path.isfile(f\"{os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_fieldcoef.nii.gz\"),\n",
    "                                    os.path.isfile(f\"{os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_fout.nii.gz\"),\n",
    "                                    os.path.isfile(f\"{os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_iout.nii.gz\"),\n",
    "                                    os.path.isfile(f\"{os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_movpar.txt\")]\n",
    " \n",
    " # Run the launch file if not all output files present:\n",
    "if not all(expected_TOPUP_out_files_exist):\n",
    "            print(f\">> Running: launch -s {launch_file} -j schonberglab -p {n_cores_TOPUP} -r inf\")\n",
    "            #os.system(f\">> Running: launch -s {launch_file} -j schonberglab -p {n_cores_TOPUP} -r inf\")\n",
    "            \n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) EDDY\n",
    "* A tool for correcting Eddy currents (and motion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average TOPUP corrected (unwarped) B0 volums (the .iout file) [for now it prints the command]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "for time in ['before', 'after']:\n",
    "      output_file_name = f\"{os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_iout_avg.nii.gz\"\n",
    "      if not os.path.isfile(f\"{output_file_name}\"):\n",
    "            print(f\">> runs: fslmaths {os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_iout -Tmean {output_file_name}\")\n",
    "            os.system(f\"fslmaths {os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_iout -Tmean {output_file_name}\")\n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run BET on the averaged B0s volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "for time in ['before', 'after']:\n",
    "      output_base_name = f\"{os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_iout_avg_brain\"\n",
    "      if not os.path.isfile(f\"{output_base_name}.nii.gz\") or not os.path.isfile(f\"{output_base_name}_mask.nii.gz\"):\n",
    "            print(f\">> runs: bet {os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_iout_avg {output_base_name} -m -f 0.2\")\n",
    "            os.system(f\"bet {os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_iout_avg {output_base_name} -m -f 0.2\")\n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create index.txt file\n",
    "This file maps the volumes in the main DTI data to the relevant line in the acqparams.txt and in the movpar.txt (assessed movement parameters from TOPUP) files\n",
    "* details in: https://www.youtube.com/watch?v=1T1cRnX7MpA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "# create an index.txt file for the EDDY\n",
    "for time in ['before', 'after']:\n",
    "    ind_to_write = 1\n",
    "    with open(os.path.join(preproc_path, subjFolder, f'{time}_index.txt'), 'w') as f:\n",
    "        for i in range(expectedVolums['AP']):   \n",
    "            if i > 0 and i in expectedB0s_indxs[f'AP_{time}']:\n",
    "                ind_to_write += 1\n",
    "            f.write(\"%s\\n\" %ind_to_write)\n",
    "print('>> COMPLETED.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run EDDY [currently printing the command]\n",
    "* Correct for eddy currents and subject movement (and taking to account the suceptibility field calculated by TOPUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eddy_openmp --imain=/Users/ranigera/Dropbox/DTI_tests/sub-120/ses-1/dwi/sub-120_ses-1_acq-ap_run-01_dwi.nii.gz \\\n",
      "          --mask=/Users/ranigera/Dropbox/DTI_tests/preproc/sub-120/topup_sub-120_AP_PA_before_b0s_iout_avg_brain_mask \\\n",
      "          --index=/Users/ranigera/Dropbox/DTI_tests/preproc/sub-120/before_index.txt\\\n",
      "          --acqp=/Users/ranigera/Dropbox/DTI_tests/preproc/sub-120/sub-120_before_acqparams.txt \\\n",
      "          --bvecs=/Users/ranigera/Dropbox/DTI_tests/sub-120/ses-1/dwi/sub-120_ses-1_acq-ap_run-01_dwi.bvec \\\n",
      "          --bvals=/Users/ranigera/Dropbox/DTI_tests/sub-120/ses-1/dwi/sub-120_ses-1_acq-ap_run-01_dwi.bval \\\n",
      "          --topup=/Users/ranigera/Dropbox/DTI_tests/preproc/sub-120/topup_sub-120_AP_PA_before_b0s \\\n",
      "          --out=/Users/ranigera/Dropbox/DTI_tests/preproc/sub-120/eddy_unwarped_images_sub-120_before \\\n",
      "          --verbose      \n",
      "eddy_openmp --imain=/Users/ranigera/Dropbox/DTI_tests/sub-120/ses-1/dwi/sub-120_ses-1_acq-ap_run-02_dwi.nii.gz \\\n",
      "          --mask=/Users/ranigera/Dropbox/DTI_tests/preproc/sub-120/topup_sub-120_AP_PA_after_b0s_iout_avg_brain_mask \\\n",
      "          --index=/Users/ranigera/Dropbox/DTI_tests/preproc/sub-120/after_index.txt\\\n",
      "          --acqp=/Users/ranigera/Dropbox/DTI_tests/preproc/sub-120/sub-120_after_acqparams.txt \\\n",
      "          --bvecs=/Users/ranigera/Dropbox/DTI_tests/sub-120/ses-1/dwi/sub-120_ses-1_acq-ap_run-02_dwi.bvec \\\n",
      "          --bvals=/Users/ranigera/Dropbox/DTI_tests/sub-120/ses-1/dwi/sub-120_ses-1_acq-ap_run-02_dwi.bval \\\n",
      "          --topup=/Users/ranigera/Dropbox/DTI_tests/preproc/sub-120/topup_sub-120_AP_PA_after_b0s \\\n",
      "          --out=/Users/ranigera/Dropbox/DTI_tests/preproc/sub-120/eddy_unwarped_images_sub-120_after \\\n",
      "          --verbose      \n"
     ]
    }
   ],
   "source": [
    "for time in ['before', 'after']:\n",
    "     print(f\"eddy_openmp --imain={scansBaseNames[f'AP_{time}']}.nii.gz \\\\\\n\\\n",
    "          --mask={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_iout_avg_brain_mask \\\\\\n\\\n",
    "          --index={os.path.join(preproc_path, subjFolder, time + '_index.txt')}\\\\\\n\\\n",
    "          --acqp={os.path.join(preproc_path, subjFolder, subjFolder + '_' + time + '_acqparams.txt')} \\\\\\n\\\n",
    "          --bvecs={scansBaseNames[f'AP_{time}']}.bvec \\\\\\n\\\n",
    "          --bvals={scansBaseNames[f'AP_{time}']}.bval \\\\\\n\\\n",
    "          --topup={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s \\\\\\n\\\n",
    "          --out={os.path.join(preproc_path, subjFolder,'eddy_unwarped_images_' + subjFolder)}_{time} \\\\\\n\\\n",
    "          --verbose \\\n",
    "     \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create EDDY launch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create the EDDY launch files (one per subject [before and after] together)\n",
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "print('> Create the EDDY launch files (one per subject [''before'' and ''after''] together)')\n",
    "\n",
    "launch_file = os.path.join(launch_files_path, 'EDDY_' + subjFolder + '_launch.txt')\n",
    "\n",
    "if not os.path.isfile(launch_file):\n",
    "    with open(launch_file, 'w') as f:\n",
    "        for time in ['before', 'after']:\n",
    "            # print(f\">> This command was written to {launch_file}:\\ntopup --imain={os.path.join(preproc_path, subjFolder,subjFolder)}_AP_PA_{time}_b0s \\\\\\n\\\n",
    "            #         --datain={os.path.join(preproc_path, subjFolder, subjFolder + f'_{time}_acqparams.txt')} \\\\\\n\\\n",
    "            #         --config=b02b0.cnf \\\\\\n\\\n",
    "            #         --out={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s \\\\\\n\\\n",
    "            #         --iout={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_iout \\\\\\n\\\n",
    "            #         --fout={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_fout \\n\\\n",
    "            #     \")\n",
    "            print(f\">> This command was written to {launch_file}:\\n{EDDY_command} --imain={scansBaseNames[f'AP_{time}']}.nii.gz \\\\\\n\\\n",
    "                --mask={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_iout_avg_brain_mask \\\\\\n\\\n",
    "                --index={os.path.join(preproc_path, subjFolder, time + '_index.txt')}\\\\\\n\\\n",
    "                --acqp={os.path.join(preproc_path, subjFolder, subjFolder + '_' + time + '_acqparams.txt')} \\\\\\n\\\n",
    "                --bvecs={scansBaseNames[f'AP_{time}']}.bvec \\\\\\n\\\n",
    "                --bvals={scansBaseNames[f'AP_{time}']}.bval \\\\\\n\\\n",
    "                --topup={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s \\\\\\n\\\n",
    "                --out={os.path.join(preproc_path, subjFolder,'eddy_unwarped_images_' + subjFolder)}_{time} \\\\\\n\\\n",
    "                --verbose \\n\\\n",
    "            \")\n",
    "            f.write(f\"{EDDY_command} --imain={scansBaseNames[f'AP_{time}']}.nii.gz --mask={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s_iout_avg_brain_mask --index={os.path.join(preproc_path, subjFolder, time + '_index.txt')} --acqp={os.path.join(preproc_path, subjFolder, subjFolder + '_' + time + '_acqparams.txt')} --bvecs={scansBaseNames[f'AP_{time}']}.bvec --bvals={scansBaseNames[f'AP_{time}']}.bval --topup={os.path.join(preproc_path, subjFolder,'topup_' + subjFolder)}_AP_PA_{time}_b0s --out={os.path.join(preproc_path, subjFolder,'eddy_unwarped_images_' + subjFolder)}_{time} --verbose\\n\")\n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run EDDY launch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Run the EDDY launch files (one per subject [before and after] together)\n",
      ">> Running: launch -s /Users/ranigera/Dropbox/DTI_analysis/launch_files/EDDY_sub-120_launch.txt -j schonberglab -p 4 -r inf\n",
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "print('> Run the EDDY launch files (one per subject [''before'' and ''after''] together)')\n",
    "\n",
    "launch_file = os.path.join(launch_files_path, 'EDDY_' + subjFolder + '_launch.txt')\n",
    "\n",
    "# check if all expected output files exist:\n",
    "expected_EDDY_out_files_exist = []\n",
    "for time in ['before', 'after']:\n",
    "    eddy_output_files = glob.glob(os.path.join(preproc_path, subjFolder, f'eddy_unwarped_images_{subjFolder}_{time}*')) # get files in a directory\n",
    "    expected_EDDY_out_files_exist.append(len(list(set(eddy_output_files))) == n_expected_EDDY_output_files) # check that the number of unique EDDY files is as expected\n",
    "\n",
    " # Run the launch file if not all output files present:\n",
    "if not all(expected_EDDY_out_files_exist):\n",
    "            print(f\">> Running: launch -s {launch_file} -j schonberglab -p {n_cores_EDDY} -r inf\")\n",
    "            os.system(f\">> Running: launch -s {launch_file} -j schonberglab -p {n_cores_EDDY} -r inf\")\n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run EDDY QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sub-120'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/ranigera/Dropbox/DTI_tests/preproc/sub-120/before_index.txt'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{os.path.join(preproc_path, subjFolder, f'{time}_index.txt')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weew\n",
      "werewr\n"
     ]
    }
   ],
   "source": [
    "print('weew\\nwerewr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in ['before', 'after']:\n",
    "    os.system(f\"eddy_quad {os.path.join(preproc_path, subjFolder, f'eddy_unwarped_images_{subjFolder}_{time}')} \\\n",
    "    -idx {os.path.join(preproc_path, subjFolder, f'{time}_index.txt')} \\\n",
    "    -par {os.path.join(preproc_path, subjFolder, subjFolder  + f'_{time}_acqparams.txt')} \\\n",
    "    -m   {os.path.join(preproc_path, subjFolder, f'topup_{subjFolder}_AP_PA_{time}_b0s_iout_avg_brain_mask.nii.gz')} \\\n",
    "    -b   {scansBaseNames[f'AP_{time}'] + '.bval'}\")\n",
    "\n",
    "#f{os.path.join(preproc_path, subjFolder, f'eddy_unwarped_images_{subjFolder}_{time}')} -idx {time}_index.txt -par sub-120_before_acqparams.txt -m topup_sub-120_AP_PA_before_b0s_iout_avg_brain_mask.nii.gz -b /Users/ranigera/Dropbox/DTI_tests/sub-120/ses-1/dwi/sub-120_ses-1_acq-ap_run-01_dwi.bval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ranigera/Dropbox/DTI_tests/sub-120/ses-1/dwi/sub-120_ses-1_acq-ap_run-01_dwi.bval'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scansBaseNames[f'AP_{time}'] + '.bval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ranigera/Dropbox/DTI_tests/sub-120/ses-1/dwi/sub-120_ses-1_acq-ap_run-01_dwi'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scansBaseNames[f'AP_{time}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7504252075d046cca664b6be5d41f96883e23643544d29c440209b91eeb6bd3e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
